{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkful - 3.3.4 - Challenge - Logistic, Ridge and Lasso Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9289\n",
      "9289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"df[variables] = df[variables].astype(int)\\ndf['State'] = df['State'].map(lambda x: x.title())\\ndf.head()\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "sns.set(style=\"white\", context=\"talk\")\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\")\n",
    " \n",
    "#Read file and remove commas from numbers (currently strings)\n",
    "df2 = pd.read_csv('Data/Table_8_Offenses_Known_to_Law_Enforcement_by_State_by_City_2013_v2.csv')\n",
    "print(len(df2))\n",
    "#df = df2.iloc[:,:].dropna()\n",
    "df = df2.dropna()\n",
    "print(len(df))\n",
    "\n",
    "variables = ['Population','Murder','Violent crime','Robbery','Aggravated assault','Arson',\n",
    "            'Property crime','Burglary','Larceny-theft','Motor vehicle theft','Rape']\n",
    "'''df[variables] = df[variables].astype(int)\n",
    "df['State'] = df['State'].map(lambda x: x.title())\n",
    "df.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create An Outcome Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = df['Population']\n",
    "df['Property Crime Per Capita'] = df['Property crime']/population\n",
    "prop_crime_pc = df['Property Crime Per Capita']\n",
    "\n",
    "is_safe = []\n",
    "for j in range(len(prop_crime_pc)):\n",
    "    if prop_crime_pc[j]<0.05:\n",
    "        is_safe.append(0)\n",
    "    else:\n",
    "        is_safe.append(1)\n",
    "df['Is Safe'] = pd.Series(is_safe, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating New Features**\n",
    "\n",
    "We currently have 9 features, so I created 6 more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variable for murder per capita and append to dataframe\n",
    "df['Robbery Per Capita'] = df['Robbery']/population\n",
    "df['Murder Per Capita'] = df['Murder']/population\n",
    "df['Robbery-Murder'] = df['Robbery'] * df['Murder']\n",
    "df['Robbery-Larceny'] = df['Robbery'] * df['Larceny-theft']\n",
    "\n",
    "# Create feature for region\n",
    "Region = []\n",
    "Northeast = ['Connecticut','Maine','Massachusetts','New Hampshire','Rhode Island', \n",
    "             'Vermont','New Jersey','New York','Pennsylvania']\n",
    "Midwest = ['Illinois','Indiana','Michigan','Ohio','Wisconsin','Iowa','Kansas', \n",
    "           'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota']\n",
    "South = ['Delaware','Florida','Georgia','Maryland','North Carolina','South Carolina',\n",
    "        'Virginia','District of Columbia','West Virginia','Alabama','Kentucky', \n",
    "        'Mississippi','Tennessee','Arkansas','Louisiana','Oklahoma','Texas']\n",
    "West = ['Arizona','Colorado','Idaho','Montana','Nevada','New Mexico','Utah','Wyoming', \n",
    "        'Alaska','California','Hawaii','Oregon','Washington']\n",
    "\n",
    "for i in range(len(df['State'])):\n",
    "    if df['State'][i] in Northeast:\n",
    "        Region.append(0)\n",
    "    elif df['State'][i] in Midwest:\n",
    "        Region.append(1)\n",
    "    elif df['State'][i] in South:\n",
    "        Region.append(2)\n",
    "    else:\n",
    "        Region.append(3)\n",
    "\n",
    "df['Region'] = pd.Series(Region, index=df.index) \n",
    "\n",
    "# Create feature for city size\n",
    "Large_City = []\n",
    "for j in range(len(df['Population'])):\n",
    "    if df['Population'][j]<30000:\n",
    "        Large_City.append(0)\n",
    "    elif df['Population'][j]<60000:\n",
    "        Large_City.append(1)\n",
    "    else:\n",
    "        Large_City.append(2)\n",
    "df['Large City'] = pd.Series(Large_City, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Murder','Violent crime','Robbery','Aggravated assault','Arson',\n",
    "            'Burglary','Larceny-theft','Motor vehicle theft','Rape',\n",
    "            'Robbery Per Capita','Murder Per Capita','Large City','Region',\n",
    "           'Robbery-Murder','Robbery-Larceny']\n",
    "# Need more features + Region excluded because keyerror could not be converted to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[  1.47582212e-04   8.43159639e-05   2.25665878e-03  -9.22608131e-04\n",
      "   -2.07096847e-04   3.33940722e-04  -6.06692024e-04   7.17723451e-04\n",
      "   -1.44598575e-03  -1.16462262e-07  -1.99549685e-08  -2.11465752e-04\n",
      "   -5.81230526e-03  -6.51500577e-06   3.76759897e-08]]\n",
      "[-0.00193744]\n",
      "R^2 =  0.89955861772\n",
      "Cross Validation, CV = 10:\n",
      "[ 0.71182796  0.89247312  0.90204521  0.89666308  0.90312164  0.90204521\n",
      "  0.90419806  0.90193966  0.90517241  0.90409483]\n",
      "Unweighted Accuracy: 0.88 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "# Declare a logistic regression classifier.\n",
    "# Parameter regularization coefficient C described above. L2 = ridge, L2 = lasso\n",
    "lr = LogisticRegression(C=1e9)\n",
    "y1 = is_safe\n",
    "X = df[features]\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(X, y1)\n",
    "\n",
    "# Display.\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print(fit.intercept_)\n",
    "pred_y_sklearn = lr.predict(X)\n",
    "\n",
    "print(\"R^2 = \",lr.score(X, y1))\n",
    "score_log = cross_val_score(lr, X, y1, cv=10)\n",
    "print(\"Cross Validation, CV = 10:\")\n",
    "print(score_log)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score_log.mean(), score_log.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection of regularization parameter for ridge and lasso regression**\n",
    "\n",
    "After evaluating the impact of alpha on the R^2 value for both ridge and lasso regression (below), it appears that the R^2 value for the ridge regression model decreases as alpha increases (which is expected), whereas the R^2 value for lasso regression is consistently zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphaStep = []\n",
    "R2_ridge = []\n",
    "R2_lasso = []\n",
    "Alph_1 = 0.1\n",
    "i = 1\n",
    "\n",
    "while i < 100:\n",
    "    \n",
    "    #Ridge Regression\n",
    "    ridgeregr = linear_model.Ridge(alpha=Alph_1, fit_intercept=False) \n",
    "    ridgeregr.fit(X, y)\n",
    "    R2_ridge.append(ridgeregr.score(X, y))\n",
    "    \n",
    "    # Lasso Regression\n",
    "    lass = linear_model.Lasso(alpha=Alph_1)\n",
    "    lassfit = lass.fit(X, y)\n",
    "    R2_lasso.append(lass.score(X, y))\n",
    "    \n",
    "    # Iterate\n",
    "    alphaStep.append(Alph_1)\n",
    "    i += 1\n",
    "    Alph_1 += .1\n",
    "\n",
    "# Plot Results\n",
    "plt.scatter(alphaStep,R2_ridge,color='blue')\n",
    "plt.scatter(alphaStep,R2_lasso,color='red')\n",
    "plt.legend(['Ridge', 'Lasso'])\n",
    "plt.title('R-squared vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('R-squared')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[ -1.53813819e-03   1.34210820e-04  -2.71213286e-04  -1.57613821e-04\n",
      "  -7.15126030e-05  -6.90500574e-06   1.82438809e-05   2.56619242e-06\n",
      "  -2.25874993e-04   3.97334761e+01  -7.56341929e-03  -1.88510070e-02\n",
      "   4.90981114e-03   7.49902434e-07  -1.31392911e-09]\n",
      "R^2 =  0.389602411358\n",
      "Cross Validation, CV = 10:\n",
      "[ 0.78300504  0.00592372 -0.28292164 -0.14900114 -1.54136874 -1.075824\n",
      " -0.2061249  -2.16813826 -1.10920803 -0.00594016]\n",
      "Unweighted Accuracy: -0.57 (+/- 1.66)\n"
     ]
    }
   ],
   "source": [
    "Alph_1 = .5\n",
    "y = prop_crime_pc\n",
    "\n",
    "#Ridge Regression\n",
    "ridgeregr = linear_model.Ridge(alpha=Alph_1, fit_intercept=False) \n",
    "fit_r = ridgeregr.fit(X, y)\n",
    "print('Coefficients')\n",
    "print(fit_r.coef_)\n",
    "print(\"R^2 = \",ridgeregr.score(X, y))\n",
    "score_ridge = cross_val_score(ridgeregr, X, y, cv=10)\n",
    "print(\"Cross Validation, CV = 10:\")\n",
    "print(score_ridge)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score_ridge.mean(), score_ridge.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[ -0.00000000e+00   0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00  -2.49093442e-05   1.16757278e-05  -1.45370276e-06\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00   4.14032423e-08  -5.61461841e-10]\n",
      "R^2 =  0.00012842426557\n",
      "Cross Validation, CV = 10:\n",
      "[-0.00495614 -0.00119499 -0.22466246 -0.15923292 -0.28307245 -0.65880693\n",
      " -0.15633098 -0.77624722 -0.44986865 -0.15902156]\n",
      "Unweighted Accuracy: -0.29 (+/- 0.50)\n"
     ]
    }
   ],
   "source": [
    "Alph_1 = 0.1\n",
    "# Lasso Regression\n",
    "lass = linear_model.Lasso(alpha=Alph_1)\n",
    "lassfit = lass.fit(X, y)\n",
    "print('Coefficients')\n",
    "print(lassfit.coef_)\n",
    "print(\"R^2 = \",lass.score(X, y))\n",
    "score_lass = cross_val_score(lass, X, y, cv=10)\n",
    "print(\"Cross Validation, CV = 10:\")\n",
    "print(score_lass)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score_lass.mean(), score_lass.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "After reviewing all three models, it appears that logistic regression is the best model, which is due to its higher R^2 and average cross validation scores. A summary of these values is below: \n",
    "* Logistic Regression: R^2 = 0.90, Avg. CV = 0.88 +/- 0.11\n",
    "* Ridge Regression: R^2 = 0.39, Avg. CV = -0.57 +/- 1.66\n",
    "* Lasso Regression: R^2 = 0.000128, Avg. CV = -0.29 +/- 0.50\n",
    "\n",
    "It should be noted that the logistic regression is predicting a categorical variable, whereas the ridge and lasso regression models are predicting continuous variables, and therefore this might not be an appropriate comparison.\n",
    "\n",
    "To build these models, features were chosen which were speculated to be related to the outcome variables of whether or not the city was safe (for the logistic regression model) or what the predicted property crime was per capita (for the ridge and lasso regression models).\n",
    "\n",
    "For the ridge regression model, I chose alpha = 0.5. This decreased the overall R^2 value but increased the average cross validation score. Even when the cross-validation score was optimized, however, the unweighted accuracy was -0.57, which is not a great score.\n",
    "\n",
    "For the lasso regression model, I chose alpha = 0.1, however, the R^2 is essentially 0 for all values of alpha, which calls into question how valuable alpha is to the model (and therefore how appropriate this model is for predicting this dataset). The model also either assigned coefficients of zero or near-zero to almost all of the variables, essentially making the outcome zero for all input variables.\n",
    "\n",
    "Regression as a modeling approach can be great for making predictions, but the explanatory power of the models is questionable as the number of features is increased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
