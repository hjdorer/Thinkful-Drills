{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkful - 4.4.5 - Challenge - Build Own NLP Model\n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Thank, you, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Thank, you, all, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Thank, you, !)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Sentence  Speaker\n",
       "0                 (Thank, you, .)  Clinton\n",
       "1       (Thank, you, so, much, .)  Clinton\n",
       "2                 (Thank, you, .)  Clinton\n",
       "3  (Thank, you, all, so, much, .)  Clinton\n",
       "4                 (Thank, you, !)  Clinton"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "filename1 = 'Data/Trump.txt'\n",
    "file1 = open(filename1,'rt')\n",
    "trump = file1.read()\n",
    "file1.close()\n",
    "\n",
    "filename2 = 'Data/Clinton.txt'\n",
    "file2 = open(filename2,'rt')\n",
    "clinton = file2.read()\n",
    "file2.close()\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "trump = re.sub(r'Chapter \\d+', '', trump)\n",
    "clinton = re.sub(r'CHAPTER .*', '', clinton)\n",
    "    \n",
    "clinton = text_cleaner(clinton)\n",
    "trump = text_cleaner(trump)\n",
    "\n",
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "clinton_doc = nlp(clinton)\n",
    "trump_doc = nlp(trump)\n",
    "\n",
    "# Group into sentences.\n",
    "clinton_sents = [[sent, \"Clinton\"] for sent in clinton_doc.sents]\n",
    "trump_sents = [[sent, \"Trump\"] for sent in trump_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(clinton_sents + trump_sents)\n",
    "sentences.columns = ['Sentence','Speaker']\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25106, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique to Clinton: {'respect', 'fair', 'meet', 'kind', 'senator', 'small', 'hope', 'clean', 'young', 'health', 'student', 'today', 'raise', 'child', 'donald', 'issue', 'opponent', 'serve', 'election', 'school', 'americans', 'man', 'strong', 'future', 'chance', 'debt', 'community', 'national', 'try', 'live', 'plan', 'home', 'worker', 'opportunity', 'create', 'life', 'education', 'high', 'ahead', 'service', 'sure', 'able', 'republicans', 'support', 'real', 'economy', 'help', 'value', 'nuclear', 'feel', 'to', 'campaign', \"'s\", 'stand', 'college', 'family', 'kid'}\n",
      "\n",
      "Unique to Trump: {'mexico', 'amazing', 'immigration', 'smart', 'guy', 'watch', 'iran', 'dollar', 'border', 'use', 'ago', 'remember', 'hillary', 'week', 'probably', 'problem', 'trade', 'trillion', 'company', 'in', 'little', 'military', 'incredible', 'okay', 'what', 'clinton', 'spend', 'billion', 'understand', 'how', 'second', 'politician', 'anybody', 'iowa', 'do', 'no', 'poll', 'kill', 'thousand', '’', 'folk', 'number', 'somebody', 'send', 'money', 'tremendous', 'oh', 'china', '’s', 'nice', 'bad', 'different', 'win', 'wall', 'tough', 'a', 'deal'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "clinton_lemma_freq = lemma_frequencies(clinton_doc, include_stop=False).most_common(150)\n",
    "trump_lemma_freq = lemma_frequencies(trump_doc, include_stop=False).most_common(150)\n",
    "#print('\\nClinton Most Frequent:', clinton_lemma_freq)\n",
    "#print('\\nTrump Most Frequent:', trump_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "clinton_lemma_common = [pair[0] for pair in clinton_lemma_freq]\n",
    "trump_lemma_common = [pair[0] for pair in trump_lemma_freq]\n",
    "print('\\nUnique to Clinton:', set(clinton_lemma_common) - set(trump_lemma_common))\n",
    "print('\\nUnique to Trump:', set(trump_lemma_common) - set(clinton_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117523\n",
      "6655\n",
      "168100\n",
      "6538\n",
      "Clinton's sentences were 60.1% longer than Trump's while campaigning.\n",
      "Clinton's vocabulary was 45.6% larger than Trump's while campaigning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Clinton</th>\n",
       "      <th>Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of Words</td>\n",
       "      <td>132128</td>\n",
       "      <td>196418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of Sentences</td>\n",
       "      <td>7428</td>\n",
       "      <td>17678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Words Per Sentence</td>\n",
       "      <td>17.8</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vocab Variability</td>\n",
       "      <td>0.05663</td>\n",
       "      <td>0.03889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Parameter  Clinton    Trump\n",
       "0      Number of Words   132128   196418\n",
       "1  Number of Sentences     7428    17678\n",
       "2   Words Per Sentence     17.8     11.1\n",
       "3    Vocab Variability  0.05663  0.03889"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinton_total_words = [token for token in clinton_doc if not token.is_punct]\n",
    "trump_total_words = [token for token in trump_doc if not token.is_punct]\n",
    "\n",
    "clinton_unique_words = set([token.text for token in clinton_total_words])\n",
    "trump_unique_words = set([token.text for token in trump_total_words])\n",
    "\n",
    "print(len(clinton_total_words))\n",
    "print(len(clinton_unique_words))\n",
    "print(len(trump_total_words))\n",
    "print(len(trump_unique_words))\n",
    "\n",
    "clinton_wps = len(clinton_doc)/len(clinton_sents)\n",
    "trump_wps = len(trump_doc)/len(trump_sents)\n",
    "wps_comp = clinton_wps/trump_wps\n",
    "\n",
    "clinton_vocab = len(clinton_unique_words)/len(clinton_total_words)\n",
    "trump_vocab = len(trump_unique_words)/len(trump_total_words)\n",
    "vocab_comp = clinton_vocab/trump_vocab\n",
    "\n",
    "print((\"Clinton's sentences were {}% longer than Trump's while campaigning.\").format(\"%0.1f\" % ((wps_comp-1)*100)))\n",
    "print((\"Clinton's vocabulary was {}% larger than Trump's while campaigning.\").format(\"%0.1f\" % ((vocab_comp-1)*100)))\n",
    "\n",
    "wordsummary = np.array([['Number of Words',len(clinton_doc),len(trump_doc)],\n",
    "                        ['Number of Sentences',len(clinton_sents),len(trump_sents)],\n",
    "                        ['Words Per Sentence',\"%0.1f\" % clinton_wps,\"%0.1f\" % trump_wps],\n",
    "                        ['Vocab Variability',\"%0.5f\" % clinton_vocab,\"%0.5f\" % trump_vocab]])\n",
    "\n",
    "df_wordsummary = pd.DataFrame(wordsummary)\n",
    "df_wordsummary.columns = ['Parameter','Clinton','Trump']\n",
    "df_wordsummary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parts of Speech Bags Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCONJ', 'PART', 'ADP', 'NUM', 'X', 'PRON', 'DET', 'VERB', 'ADV', 'INTJ', 'NOUN', 'SYM', 'PUNCT', 'PROPN', 'ADJ']\n"
     ]
    }
   ],
   "source": [
    "# Utility function to create a list of the potential parts of speech\n",
    "def parts_of_speech(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    parts_speech = [token.pos_ for token in text]\n",
    "    \n",
    "    # Return all words.\n",
    "    #return [item[0] for item in Counter(parts_speech)]\n",
    "    return parts_speech\n",
    "\n",
    "# Set up the bags.\n",
    "clintonpos = parts_of_speech(clinton_doc)\n",
    "trumppos = parts_of_speech(trump_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "pos_words = list(set(clintonpos + trumppos))\n",
    "\n",
    "print(pos_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parts of Speech Counter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 5000\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "Processing row 20000\n",
      "Processing row 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>PART</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>PRON</th>\n",
       "      <th>DET</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADV</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>SYM</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, all, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, !)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CCONJ PART ADP NUM  X PRON DET VERB ADV INTJ NOUN SYM PUNCT PROPN ADJ  \\\n",
       "0     0    0   0   0  0    1   0    1   0    0    0   0     1     0   0   \n",
       "1     0    0   0   0  0    1   0    1   2    0    0   0     1     0   0   \n",
       "2     0    0   0   0  0    1   0    1   0    0    0   0     1     0   0   \n",
       "3     0    0   0   0  0    1   1    1   2    0    0   0     1     0   0   \n",
       "4     0    0   0   0  0    1   0    1   0    0    0   0     1     0   0   \n",
       "\n",
       "                         Sentence  Speaker  \n",
       "0                 (Thank, you, .)  Clinton  \n",
       "1       (Thank, you, so, much, .)  Clinton  \n",
       "2                 (Thank, you, .)  Clinton  \n",
       "3  (Thank, you, all, so, much, .)  Clinton  \n",
       "4                 (Thank, you, !)  Clinton  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a data frame with features for each part of speech corresponding to each word in our part of speech word set.\n",
    "# Each value is the count of the times the part of speech appears in each sentence.\n",
    "def pos_features(sentences, pos_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df1 = pd.DataFrame(columns=pos_words)\n",
    "    df1['Sentence'] = sentences['Sentence']\n",
    "    df1['Speaker'] = sentences['Speaker']\n",
    "    df1.loc[:, pos_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df1['Sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        pos = [token.pos_ for token in sentence]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for pos1 in pos:\n",
    "            df1.loc[i, pos1] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "    \n",
    "    return df1\n",
    "        \n",
    "# Create our data frame with features. This can take a while to run.\n",
    "pos_counts = pos_features(sentences, pos_words)\n",
    "pos_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Words Bags Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "# Set up the bags.\n",
    "clintonwords = bag_of_words(clinton_doc)\n",
    "trumpwords = bag_of_words(trump_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(clintonwords + trumpwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Counter: Word Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 5000\n",
      "Processing row 10000\n",
      "Processing row 15000\n",
      "Processing row 20000\n",
      "Processing row 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay</th>\n",
       "      <th>mosque</th>\n",
       "      <th>ride</th>\n",
       "      <th>thank</th>\n",
       "      <th>suffer</th>\n",
       "      <th>depression</th>\n",
       "      <th>leave</th>\n",
       "      <th>goodness</th>\n",
       "      <th>vast</th>\n",
       "      <th>march</th>\n",
       "      <th>...</th>\n",
       "      <th>during</th>\n",
       "      <th>fairness</th>\n",
       "      <th>moines</th>\n",
       "      <th>payment</th>\n",
       "      <th>cherish</th>\n",
       "      <th>certainly</th>\n",
       "      <th>everything</th>\n",
       "      <th>caucus</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, all, so, much, .)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Thank, you, !)</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2879 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  stay mosque ride thank suffer depression leave goodness vast march   ...     \\\n",
       "0    0      0    0     1      0          0     0        0    0     0   ...      \n",
       "1    0      0    0     1      0          0     0        0    0     0   ...      \n",
       "2    0      0    0     1      0          0     0        0    0     0   ...      \n",
       "3    0      0    0     1      0          0     0        0    0     0   ...      \n",
       "4    0      0    0     1      0          0     0        0    0     0   ...      \n",
       "\n",
       "  during fairness moines payment cherish certainly everything caucus  \\\n",
       "0      0        0      0       0       0         0          0      0   \n",
       "1      0        0      0       0       0         0          0      0   \n",
       "2      0        0      0       0       0         0          0      0   \n",
       "3      0        0      0       0       0         0          0      0   \n",
       "4      0        0      0       0       0         0          0      0   \n",
       "\n",
       "                         Sentence  Speaker  \n",
       "0                 (Thank, you, .)  Clinton  \n",
       "1       (Thank, you, so, much, .)  Clinton  \n",
       "2                 (Thank, you, .)  Clinton  \n",
       "3  (Thank, you, all, so, much, .)  Clinton  \n",
       "4                 (Thank, you, !)  Clinton  \n",
       "\n",
       "[5 rows x 2879 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df2 = pd.DataFrame(columns=common_words)\n",
    "    df2['Sentence'] = sentences['Sentence']\n",
    "    df2['Speaker'] = sentences['Speaker']\n",
    "    df2.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df2['Sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words)]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df2.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 5000 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "              \n",
    "    return df2\n",
    "\n",
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Counter: Total and Unique Word Counter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Total Word Count</th>\n",
       "      <th>Unique Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Thank, you, so, much, .)</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Thank, you, .)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Thank, you, all, so, much, .)</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Thank, you, !)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Thank, you, !)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Thank, you, all, very, ,, very, much, !)</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Thank, you, for, that, amazing, welcome, !)</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Thank, you, all, for, the, great, convention,...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(And, Chelsea, ,, thank, you, .)</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Total Word Count  \\\n",
       "0                                    (Thank, you, .)                 2   \n",
       "1                          (Thank, you, so, much, .)                 4   \n",
       "2                                    (Thank, you, .)                 2   \n",
       "3                     (Thank, you, all, so, much, .)                 5   \n",
       "4                                    (Thank, you, !)                 2   \n",
       "5                                    (Thank, you, !)                 2   \n",
       "6          (Thank, you, all, very, ,, very, much, !)                 6   \n",
       "7       (Thank, you, for, that, amazing, welcome, !)                 6   \n",
       "8  (Thank, you, all, for, the, great, convention,...                11   \n",
       "9                   (And, Chelsea, ,, thank, you, .)                 4   \n",
       "\n",
       "   Unique Word Count  \n",
       "0                  2  \n",
       "1                  4  \n",
       "2                  2  \n",
       "3                  5  \n",
       "4                  2  \n",
       "5                  2  \n",
       "6                  5  \n",
       "7                  6  \n",
       "8                 11  \n",
       "9                  4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = sentences.drop(['Speaker'],axis=1)\n",
    "total_count = []\n",
    "unique_count = []\n",
    "\n",
    "# Process each row, counting the occurrence of words in each sentence.\n",
    "for i, sentence in enumerate(df3['Sentence']):\n",
    "    \n",
    "    #print(sentence)\n",
    "    total_word_count = [token for token in sentence if not token.is_punct]\n",
    "    unique_word_count = set([token.text for token in total_word_count])\n",
    "\n",
    "    total_count.append(len(total_word_count))\n",
    "    unique_count.append(len(unique_word_count))\n",
    "        \n",
    "df3['Total Word Count'] = pd.Series(total_count, index=df3.index)\n",
    "df3['Unique Word Count'] = pd.Series(unique_count, index=df3.index)\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = word_counts['Speaker']\n",
    "X = word_counts.drop(['Sentence','Speaker'], 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words with Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.975303724358\n",
      "\n",
      "Test set score: 0.815493378473\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_train_1 = rfc.score(X_train, y_train)\n",
    "rfc_test_1 = rfc.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', rfc_train_1)\n",
    "print('\\nTest set score:', rfc_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words with Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15063, 2877) (15063,)\n",
      "Training set score: 0.8962358096\n",
      "\n",
      "Test set score: 0.850542666534\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "lr_train_1 = lr.score(X_train, y_train)\n",
    "lr_test_1 = lr.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', lr_train_1)\n",
    "print('\\nTest set score:', lr_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words with Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.79187412866\n",
      "\n",
      "Test set score: 0.776859504132\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "clf_train_1 = clf.score(X_train, y_train)\n",
    "clf_test_1 = clf.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', clf_train_1)\n",
    "print('\\nTest set score:', clf_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words with Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9097789285\n",
      "\n",
      "Test set score: 0.850542666534\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_train_1 = svm.score(X_train, y_train)\n",
    "svm_test_1 = svm.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', svm_train_1)\n",
    "print('\\nTest set score:', svm_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y1 = pos_counts['Speaker']\n",
    "X1 = pos_counts.drop(['Sentence','Speaker'], 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.945296421696\n",
      "\n",
      "Test set score: 0.715821965548\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_train_2 = rfc.score(X_train, y_train)\n",
    "rfc_test_2 = rfc.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', rfc_train_2)\n",
    "print('\\nTest set score:', rfc_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15063, 15) (15063,)\n",
      "Training set score: 0.743344619266\n",
      "\n",
      "Test set score: 0.736433336652\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "lr_train_2 = lr.score(X_train, y_train)\n",
    "lr_test_2 = lr.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', lr_train_2)\n",
    "print('\\nTest set score:', lr_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.759543251676\n",
      "\n",
      "Test set score: 0.742407647117\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "clf_train_2 = clf.score(X_train, y_train)\n",
    "clf_test_2 = clf.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', clf_train_2)\n",
    "print('\\nTest set score:', clf_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.736108344951\n",
      "\n",
      "Test set score: 0.731454744598\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_train_2 = svm.score(X_train, y_train)\n",
    "svm_test_2 = svm.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', svm_train_2)\n",
    "print('\\nTest set score:', svm_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Models: BoW, PoS and Unique Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_counts2 = pos_counts.drop(['Sentence','Speaker'], axis=1)\n",
    "df4 = df3.drop(['Sentence'], axis=1)\n",
    "df = pd.concat([pos_counts2, word_counts,df4], axis=1)\n",
    "Y2 = df['Speaker']\n",
    "X2 = df.drop(['Sentence','Speaker'], 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, Y2,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.986456881099\n",
      "\n",
      "Test set score: 0.805237478841\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_train_3 = rfc.score(X_train, y_train)\n",
    "rfc_test_3 = rfc.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', rfc_train_3)\n",
    "print('\\nTest set score:', rfc_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15063, 2894) (15063,)\n",
      "Training set score: 0.902476266348\n",
      "\n",
      "Test set score: 0.860101563278\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "lr_train_3 = lr.score(X_train, y_train)\n",
    "lr_test_3 = lr.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', lr_train_3)\n",
    "print('\\nTest set score:', lr_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.814645157007\n",
      "\n",
      "Test set score: 0.796176441302\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "clf_train_3 = clf.score(X_train, y_train)\n",
    "clf_test_3 = clf.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', clf_train_3)\n",
    "print('\\nTest set score:', clf_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.915886609573\n",
      "\n",
      "Test set score: 0.856915264363\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_train_3 = svm.score(X_train, y_train)\n",
    "svm_test_3 = svm.score(X_test, y_test)\n",
    "\n",
    "print('Training set score:', svm_train_3)\n",
    "print('\\nTest set score:', svm_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BoW</th>\n",
       "      <th>PoS</th>\n",
       "      <th>Improved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Regression</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model    BoW    PoS Improved\n",
       "0      Random Forest Classifier  0.815  0.716    0.805\n",
       "1           Logistic Regression  0.851  0.736    0.860\n",
       "2  Gradient Boosting Regression  0.777  0.742    0.796\n",
       "3       Support Vector Machines  0.851  0.731    0.857"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = np.array([['Random Forest Classifier',\"%0.3f\" % rfc_test_1,\n",
    "                      \"%0.3f\" % rfc_test_2,\"%0.3f\" % rfc_test_3],\n",
    "                  ['Logistic Regression',\"%0.3f\" % lr_test_1,\n",
    "                      \"%0.3f\" % lr_test_2,\"%0.3f\" % lr_test_3],\n",
    "                  ['Gradient Boosting Regression',\"%0.3f\" % clf_test_1,\n",
    "                      \"%0.3f\" % clf_test_2,\"%0.3f\" % clf_test_3],\n",
    "                  ['Support Vector Machines',\"%0.3f\" % svm_test_1,\n",
    "                      \"%0.3f\" % svm_test_2,\"%0.3f\" % svm_test_3]])\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary.columns = ['Model','BoW','PoS','Improved']\n",
    "\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 83.6min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 164.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'deviance', 'max_depth': 8, 'n_estimators': 100, 'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "params = {'loss':['deviance','exponential'],\n",
    "              'max_depth':[2,5,8],\n",
    "              'max_features':['log2','sqrt','auto'],\n",
    "              'n_estimators':[5,10,100]}\n",
    "\n",
    "gbc = ensemble.GradientBoostingClassifier(params)\n",
    "clf = GridSearchCV(gbc, params, cv=5, n_jobs=-1,verbose=1)\n",
    "clf.fit(X2,Y2)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared:\n",
      "0.890072687444\n",
      "[ 0.81393035  0.81781981  0.80926295  0.81175299  0.82270916]\n",
      "Weighted Accuracy: 0.82 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"loss\":\"deviance\",\n",
    "    \"max_depth\":8,\n",
    "    \"max_features\":\"auto\",\n",
    "    \"n_estimators\":100\n",
    "    }\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_test,y_test)\n",
    "\n",
    "#Inspect results\n",
    "print('\\nR-squared:')\n",
    "print(clf.score(X_test, y_test))\n",
    "score_w = cross_val_score(clf, X_test, y_test, cv=5)\n",
    "print(score_w)\n",
    "print(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
